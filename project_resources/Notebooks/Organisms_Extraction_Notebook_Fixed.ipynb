{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ab3f3b",
   "metadata": {},
   "source": [
    "# WHONET Organisms Code Extraction and Processing\n",
    "\n",
    "This notebook extracts organism codes, names, and types from the WHONET Organisms.txt file and saves them to a structured CSV file.\n",
    "\n",
    "## Steps:\n",
    "1. Load the Organisms.txt file from the local resources\n",
    "2. Process and clean the data\n",
    "3. Optionally supplement with web scraping from the WHONET Code Finder website\n",
    "4. Save the complete data to CSV with additional metadata\n",
    "5. Generate analysis and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab51218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional imports for web scraping if needed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6421d02",
   "metadata": {},
   "source": [
    "## Load WHONET Organisms.txt File\n",
    "First, we'll load the existing Organisms.txt file and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4914a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading organisms data from C:\\Personal_Projects\\Astro\\project_resources\\Organisms.txt...\n",
      "Successfully loaded 2946 organisms using pandas\n",
      "\n",
      "DataFrame contains 2946 rows and 26 columns\n",
      "\n",
      "Columns:\n",
      "['WHONET_ORG_CODE', 'REPLACED_BY', 'ORGANISM', 'TAXONOMIC_STATUS', 'COMMON', 'ORGANISM_TYPE', 'ANAEROBE', 'MORPHOLOGY', 'SUBKINGDOM_CODE', 'FAMILY_CODE', 'GENUS_GROUP', 'GENUS_CODE', 'SPECIES_GROUP', 'SEROVAR_GROUP', 'MSF_GRP_CLIN', 'SCT_CODE', 'SCT_TEXT', 'GBIF_TAXON_ID', 'GBIF_DATASET_ID', 'GBIF_TAXONOMIC_STATUS', 'KINGDOM', 'PHYLUM', 'CLASS', 'ORDER', 'FAMILY', 'GENUS']\n",
      "\n",
      "First 5 records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WHONET_ORG_CODE</th>\n",
       "      <th>REPLACED_BY</th>\n",
       "      <th>ORGANISM</th>\n",
       "      <th>TAXONOMIC_STATUS</th>\n",
       "      <th>COMMON</th>\n",
       "      <th>ORGANISM_TYPE</th>\n",
       "      <th>ANAEROBE</th>\n",
       "      <th>MORPHOLOGY</th>\n",
       "      <th>SUBKINGDOM_CODE</th>\n",
       "      <th>FAMILY_CODE</th>\n",
       "      <th>...</th>\n",
       "      <th>SCT_TEXT</th>\n",
       "      <th>GBIF_TAXON_ID</th>\n",
       "      <th>GBIF_DATASET_ID</th>\n",
       "      <th>GBIF_TAXONOMIC_STATUS</th>\n",
       "      <th>KINGDOM</th>\n",
       "      <th>PHYLUM</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>GENUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abiotrophia adiacens</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Granulicatella adiacens</td>\n",
       "      <td>3227166</td>\n",
       "      <td>7ddf754f-d193-4cc9-b351-99906754a03b</td>\n",
       "      <td>accepted</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Carnobacteriaceae</td>\n",
       "      <td>Granulicatella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abiotrophia defectiva</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Abiotrophia defectiva</td>\n",
       "      <td>3227009</td>\n",
       "      <td>7ddf754f-d193-4cc9-b351-99906754a03b</td>\n",
       "      <td>accepted</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Aerococcaceae</td>\n",
       "      <td>Abiotrophia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abiotrophia sp.</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Abiotrophia species</td>\n",
       "      <td>3227008</td>\n",
       "      <td>7ddf754f-d193-4cc9-b351-99906754a03b</td>\n",
       "      <td>accepted</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Aerococcaceae</td>\n",
       "      <td>Abiotrophia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absidia corimbifera</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2558326</td>\n",
       "      <td>7ddf754f-d193-4cc9-b351-99906754a03b</td>\n",
       "      <td>accepted</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Zygomycota</td>\n",
       "      <td>Mucoromycetes</td>\n",
       "      <td>Mucorales</td>\n",
       "      <td>Lichtheimiaceae</td>\n",
       "      <td>Lichtheimia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absidia sp.</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Absidia species</td>\n",
       "      <td>2558224</td>\n",
       "      <td>7ddf754f-d193-4cc9-b351-99906754a03b</td>\n",
       "      <td>accepted</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Zygomycota</td>\n",
       "      <td>Mucoromycetes</td>\n",
       "      <td>Mucorales</td>\n",
       "      <td>Cunninghamellaceae</td>\n",
       "      <td>Absidia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  WHONET_ORG_CODE REPLACED_BY               ORGANISM TAXONOMIC_STATUS COMMON  \\\n",
       "0             saj         NaN   Abiotrophia adiacens                O    NaN   \n",
       "1             sdf         NaN  Abiotrophia defectiva                C    NaN   \n",
       "2             abi         NaN        Abiotrophia sp.                C    NaN   \n",
       "3             acf         NaN    Absidia corimbifera                O    NaN   \n",
       "4             abs         NaN            Absidia sp.                C    NaN   \n",
       "\n",
       "  ORGANISM_TYPE ANAEROBE  MORPHOLOGY SUBKINGDOM_CODE FAMILY_CODE  ...  \\\n",
       "0             +      NaN         NaN               +         NaN  ...   \n",
       "1             +      NaN         NaN               +         NaN  ...   \n",
       "2             +      NaN         NaN               +         NaN  ...   \n",
       "3             f      NaN         NaN             NaN         NaN  ...   \n",
       "4             f      NaN         NaN             NaN         NaN  ...   \n",
       "\n",
       "                  SCT_TEXT GBIF_TAXON_ID  \\\n",
       "0  Granulicatella adiacens       3227166   \n",
       "1    Abiotrophia defectiva       3227009   \n",
       "2      Abiotrophia species       3227008   \n",
       "3                      NaN       2558326   \n",
       "4          Absidia species       2558224   \n",
       "\n",
       "                        GBIF_DATASET_ID GBIF_TAXONOMIC_STATUS   KINGDOM  \\\n",
       "0  7ddf754f-d193-4cc9-b351-99906754a03b              accepted  Bacteria   \n",
       "1  7ddf754f-d193-4cc9-b351-99906754a03b              accepted  Bacteria   \n",
       "2  7ddf754f-d193-4cc9-b351-99906754a03b              accepted  Bacteria   \n",
       "3  7ddf754f-d193-4cc9-b351-99906754a03b              accepted     Fungi   \n",
       "4  7ddf754f-d193-4cc9-b351-99906754a03b              accepted     Fungi   \n",
       "\n",
       "       PHYLUM          CLASS            ORDER              FAMILY  \\\n",
       "0  Firmicutes        Bacilli  Lactobacillales   Carnobacteriaceae   \n",
       "1  Firmicutes        Bacilli  Lactobacillales       Aerococcaceae   \n",
       "2  Firmicutes        Bacilli  Lactobacillales       Aerococcaceae   \n",
       "3  Zygomycota  Mucoromycetes        Mucorales     Lichtheimiaceae   \n",
       "4  Zygomycota  Mucoromycetes        Mucorales  Cunninghamellaceae   \n",
       "\n",
       "            GENUS  \n",
       "0  Granulicatella  \n",
       "1     Abiotrophia  \n",
       "2     Abiotrophia  \n",
       "3     Lichtheimia  \n",
       "4         Absidia  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File path to the Organisms.txt file\n",
    "organisms_file = r'C:\\Personal_Projects\\Astro\\project_resources\\Organisms.txt'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(organisms_file):\n",
    "    print(f\"Error: File {organisms_file} not found!\")\n",
    "else:\n",
    "    print(f\"Loading organisms data from {organisms_file}...\")\n",
    "    \n",
    "    # Load the file with tab delimiter\n",
    "    try:\n",
    "        # First attempt to read with pandas\n",
    "        df_organisms = pd.read_csv(organisms_file, sep='\\t')\n",
    "        print(f\"Successfully loaded {len(df_organisms)} organisms using pandas\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading with pandas: {str(e)}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Alternative approach: read line by line\n",
    "        try:\n",
    "            with open(organisms_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Get header from first line\n",
    "            header = lines[0].strip().split('\\t')\n",
    "            \n",
    "            # Process remaining lines\n",
    "            data = []\n",
    "            for line in lines[1:]:\n",
    "                values = line.strip().split('\\t')\n",
    "                # Ensure values match header length\n",
    "                while len(values) < len(header):\n",
    "                    values.append('')\n",
    "                data.append(values[:len(header)])  # Truncate if longer than header\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df_organisms = pd.DataFrame(data, columns=header)\n",
    "            print(f\"Successfully loaded {len(df_organisms)} organisms using manual approach\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with alternative approach: {str(e2)}\")\n",
    "            df_organisms = pd.DataFrame()  # Create empty DataFrame as fallback\n",
    "            \n",
    "# Display the DataFrame structure\n",
    "if not df_organisms.empty:\n",
    "    print(f\"\\nDataFrame contains {len(df_organisms)} rows and {len(df_organisms.columns)} columns\")\n",
    "    print(\"\\nColumns:\")\n",
    "    print(list(df_organisms.columns))\n",
    "    print(\"\\nFirst 5 records:\")\n",
    "    display(df_organisms.head())\n",
    "else:\n",
    "    print(\"Failed to load organisms data from file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d5b58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and processing organism data from text file...\n",
      "Processed 2946 organisms successfully from text file\n",
      "\n",
      "Sample of processed data from text file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGANISM_CODE</th>\n",
       "      <th>ORGANISM_NAME</th>\n",
       "      <th>ORGANISM_TYPE</th>\n",
       "      <th>COMMON</th>\n",
       "      <th>ORGANISM_TYPE_DESCRIPTION</th>\n",
       "      <th>IS_COMMON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAJ</td>\n",
       "      <td>Abiotrophia adiacens</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td>Gram-positive</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDF</td>\n",
       "      <td>Abiotrophia defectiva</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td>Gram-positive</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abiotrophia sp.</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td>Gram-positive</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACF</td>\n",
       "      <td>Absidia corimbifera</td>\n",
       "      <td>f</td>\n",
       "      <td></td>\n",
       "      <td>Fungus</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABS</td>\n",
       "      <td>Absidia sp.</td>\n",
       "      <td>f</td>\n",
       "      <td></td>\n",
       "      <td>Fungus</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGANISM_CODE          ORGANISM_NAME ORGANISM_TYPE COMMON  \\\n",
       "0           SAJ   Abiotrophia adiacens             +          \n",
       "1           SDF  Abiotrophia defectiva             +          \n",
       "2           ABI        Abiotrophia sp.             +          \n",
       "3           ACF    Absidia corimbifera             f          \n",
       "4           ABS            Absidia sp.             f          \n",
       "\n",
       "  ORGANISM_TYPE_DESCRIPTION IS_COMMON  \n",
       "0             Gram-positive        No  \n",
       "1             Gram-positive        No  \n",
       "2             Gram-positive        No  \n",
       "3                    Fungus        No  \n",
       "4                    Fungus        No  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the organism data from file\n",
    "\n",
    "# Create a clean DataFrame with the columns we need\n",
    "if not df_organisms.empty:\n",
    "    print(\"Cleaning and processing organism data from text file...\")\n",
    "    \n",
    "    # Save original data count for reference\n",
    "    original_count = len(df_organisms)\n",
    "    \n",
    "    # Function to map organism type codes to full descriptions\n",
    "    def map_organism_type(code):\n",
    "        type_map = {\n",
    "            '+': 'Gram-positive',\n",
    "            '-': 'Gram-negative',\n",
    "            'a': 'Anaerobe',\n",
    "            'f': 'Fungus',\n",
    "            'm': 'Mycobacteria',\n",
    "            'b': 'Bacteria',\n",
    "            'w': 'Other'\n",
    "        }\n",
    "        return type_map.get(str(code), 'Unknown')\n",
    "    \n",
    "    # Create new DataFrame with selected columns\n",
    "    organism_cols = ['WHONET_ORG_CODE', 'ORGANISM', 'ORGANISM_TYPE', 'COMMON']\n",
    "    \n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in organism_cols if col not in df_organisms.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns in the file: {missing_cols}\")\n",
    "        # Create dictionary with available columns\n",
    "        df_clean = pd.DataFrame()\n",
    "        for col in organism_cols:\n",
    "            if col in df_organisms.columns:\n",
    "                df_clean[col] = df_organisms[col]\n",
    "            else:\n",
    "                df_clean[col] = ''\n",
    "    else:\n",
    "        df_clean = df_organisms[organism_cols].copy()\n",
    "    \n",
    "    # Rename columns to standardized names\n",
    "    df_clean = df_clean.rename(columns={\n",
    "        'WHONET_ORG_CODE': 'ORGANISM_CODE',\n",
    "        'ORGANISM': 'ORGANISM_NAME'\n",
    "    })\n",
    "    \n",
    "    # Add expanded organism type description\n",
    "    if 'ORGANISM_TYPE' in df_clean.columns:\n",
    "        df_clean['ORGANISM_TYPE_DESCRIPTION'] = df_clean['ORGANISM_TYPE'].apply(map_organism_type)\n",
    "    else:\n",
    "        df_clean['ORGANISM_TYPE_DESCRIPTION'] = 'Unknown'\n",
    "    \n",
    "    # Add common organism flag if available\n",
    "    if 'COMMON' in df_clean.columns:\n",
    "        df_clean['IS_COMMON'] = df_clean['COMMON'].apply(lambda x: 'Yes' if x == 'X' else 'No')\n",
    "    else:\n",
    "        df_clean['IS_COMMON'] = 'Unknown'\n",
    "    \n",
    "    # Clean up\n",
    "    df_clean = df_clean.fillna('')\n",
    "    \n",
    "    # Ensure all codes are uppercase\n",
    "    if 'ORGANISM_CODE' in df_clean.columns:\n",
    "        df_clean['ORGANISM_CODE'] = df_clean['ORGANISM_CODE'].str.upper()\n",
    "    \n",
    "    # Store the original file data in a separate variable for safekeeping\n",
    "    df_file_data = df_clean.copy()\n",
    "    \n",
    "    print(f\"Processed {len(df_clean)} organisms successfully from text file\")\n",
    "    \n",
    "    # Show sample of processed data\n",
    "    print(\"\\nSample of processed data from text file:\")\n",
    "    display(df_clean.head())\n",
    "else:\n",
    "    print(\"No data to process from text file.\")\n",
    "    df_clean = pd.DataFrame()\n",
    "    df_file_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e85b9c",
   "metadata": {},
   "source": [
    "## Supplement with Web Scraping (Optional)\n",
    "We can supplement our data with web scraping from WHONET Code Finder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36bad176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Web scraping disabled. Using only file data.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Web scraping for organism data as a supplement to file data\n",
    "\n",
    "# Set this to True if you want to attempt web scraping\n",
    "do_web_scraping = False  \n",
    "\n",
    "# Initialize lists to store web-scraped data\n",
    "organism_codes_web = []\n",
    "organism_names_web = []\n",
    "\n",
    "if do_web_scraping:\n",
    "    print(\"Attempting to supplement data from WHONET Code Finder website...\")\n",
    "    \n",
    "    # First, install required packages if needed\n",
    "    !pip install selenium webdriver_manager pandas beautifulsoup4\n",
    "\n",
    "    try:\n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "        # Initialize the browser\n",
    "        print(\"Setting up Chrome driver...\")\n",
    "        driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=chrome_options\n",
    "        )\n",
    "\n",
    "        # Navigate to the website\n",
    "        url = 'https://qaapt.com/whonet/code/finder'\n",
    "        print(f\"Accessing {url}...\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        print(\"Waiting for page to load...\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'select[name=\"code_type\"]'))\n",
    "        )\n",
    "\n",
    "        # Select 'Organism' from dropdown\n",
    "        print(\"Selecting 'Organism' from dropdown...\")\n",
    "        dropdown = driver.find_element(By.CSS_SELECTOR, 'select[name=\"code_type\"]')\n",
    "        for option in dropdown.find_elements(By.TAG_NAME, 'option'):\n",
    "            if option.text.strip().lower() == 'organism':\n",
    "                option.click()\n",
    "                break\n",
    "\n",
    "        # Wait for the search input to become available\n",
    "        search_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'input[type=\"search\"]'))\n",
    "        )\n",
    "\n",
    "        # Click the search button to get all results\n",
    "        search_button = driver.find_element(By.CSS_SELECTOR, 'button.btn-danger')\n",
    "        search_button.click()\n",
    "\n",
    "        # Wait for results to load\n",
    "        print(\"Waiting for results to load...\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Check if table is present\n",
    "        try:\n",
    "            table = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'table.datatable'))\n",
    "            )\n",
    "            print(\"Found results table.\")\n",
    "        except:\n",
    "            print(\"Could not find results table.\")\n",
    "            raise Exception(\"Table not found\")\n",
    "\n",
    "        # Extract table content\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, 'table.datatable tbody tr')\n",
    "        print(f\"Found {len(rows)} organism entries.\")\n",
    "\n",
    "        # Process each row\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(cells) >= 2:\n",
    "                code = cells[0].text.strip()\n",
    "                name = cells[1].text.strip()\n",
    "                if code and name:  # Only add if both values are non-empty\n",
    "                    organism_codes_web.append(code)\n",
    "                    organism_names_web.append(name)\n",
    "\n",
    "        print(f\"Successfully extracted {len(organism_codes_web)} organisms from web.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during web scraping: {str(e)}\")\n",
    "        \n",
    "        # Try fallback method with direct HTTP request\n",
    "        try:\n",
    "            print(\"\\nAttempting fallback method with direct HTTP request...\")\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get('https://qaapt.com/whonet/code/finder', headers=headers)\n",
    "            print(f\"HTTP Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Look for any table that might contain organism data\n",
    "                tables = soup.find_all('table')\n",
    "                print(f\"Found {len(tables)} tables on the page.\")\n",
    "                \n",
    "                # Parse tables if any found\n",
    "                if len(tables) > 0:\n",
    "                    for table in tables:\n",
    "                        rows = table.find_all('tr')\n",
    "                        for row in rows[1:]:  # Skip header row\n",
    "                            cells = row.find_all('td')\n",
    "                            if len(cells) >= 2:\n",
    "                                code = cells[0].get_text().strip()\n",
    "                                name = cells[1].get_text().strip()\n",
    "                                if code and name:\n",
    "                                    organism_codes_web.append(code)\n",
    "                                    organism_names_web.append(name)\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback approach also failed: {str(e2)}\")\n",
    "            \n",
    "            # Add some common organisms as fallback if all web approaches fail\n",
    "            if len(organism_codes_web) == 0:\n",
    "                print(\"Using predefined list of common WHONET organism codes as fallback.\")\n",
    "                \n",
    "                common_organisms = [\n",
    "                    (\"aba\", \"Acinetobacter baumannii\"),\n",
    "                    (\"eco\", \"Escherichia coli\"),\n",
    "                    (\"kpn\", \"Klebsiella pneumoniae\"),\n",
    "                    (\"pae\", \"Pseudomonas aeruginosa\"),\n",
    "                    (\"sau\", \"Staphylococcus aureus\")\n",
    "                ]\n",
    "                \n",
    "                # Add these to our web data lists\n",
    "                for code, name in common_organisms:\n",
    "                    organism_codes_web.append(code.upper())\n",
    "                    organism_names_web.append(name)\n",
    "                    \n",
    "                print(f\"Added {len(common_organisms)} common organisms as fallback.\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            if 'driver' in locals():\n",
    "                driver.quit()\n",
    "                print(\"Browser closed.\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create DataFrame from web-scraped data\n",
    "    if len(organism_codes_web) > 0:\n",
    "        df_web = pd.DataFrame({\n",
    "            'ORGANISM_CODE': [code.upper() for code in organism_codes_web],\n",
    "            'ORGANISM_NAME': organism_names_web\n",
    "        })\n",
    "        \n",
    "        # Add placeholder values for web data\n",
    "        df_web['ORGANISM_TYPE'] = ''\n",
    "        df_web['ORGANISM_TYPE_DESCRIPTION'] = 'Unknown'\n",
    "        df_web['IS_COMMON'] = 'No'\n",
    "        df_web['DATA_SOURCE'] = 'Web'\n",
    "        \n",
    "        # Display summary of web data\n",
    "        print(f\"\\nCreated web DataFrame with {len(df_web)} organisms\")\n",
    "        display(df_web.head())\n",
    "        \n",
    "        # Now merge with the file data (if any) - but don't overwrite the original!\n",
    "        if not df_file_data.empty:\n",
    "            # Find web data not in file data\n",
    "            file_codes = set(df_file_data['ORGANISM_CODE'].str.upper())\n",
    "            new_web_data = df_web[~df_web['ORGANISM_CODE'].isin(file_codes)]\n",
    "            \n",
    "            if len(new_web_data) > 0:\n",
    "                print(f\"\\nFound {len(new_web_data)} new organisms from web not in file data\")\n",
    "                \n",
    "                # Combine with file data\n",
    "                df_combined = pd.concat([df_file_data, new_web_data], ignore_index=True)\n",
    "                print(f\"Combined dataset now has {len(df_combined)} organisms\")\n",
    "                \n",
    "                # Update df_clean to use this combined dataset\n",
    "                df_clean = df_combined.copy()\n",
    "            else:\n",
    "                print(\"\\nNo new organisms found from web - keeping only file data\")\n",
    "        else:\n",
    "            # If no file data, just use web data\n",
    "            print(\"\\nNo file data available, using only web-scraped data\")\n",
    "            df_clean = df_web.copy()\n",
    "    else:\n",
    "        print(\"\\nNo web data extracted. Using only file data.\")\n",
    "else:\n",
    "    print(\"\\nWeb scraping disabled. Using only file data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecda88",
   "metadata": {},
   "source": [
    "## Save Final Organism Data\n",
    "Now we'll save the processed data to CSV and Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da517532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing to save 2946 organisms to files...\n",
      "Successfully saved 2946 organisms to:\n",
      " - CSV: C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.csv\n",
      " - Excel: C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.xlsx\n",
      "\n",
      "Sample of final dataset:\n",
      "Successfully saved 2946 organisms to:\n",
      " - CSV: C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.csv\n",
      " - Excel: C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.xlsx\n",
      "\n",
      "Sample of final dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGANISM_CODE</th>\n",
       "      <th>ORGANISM_NAME</th>\n",
       "      <th>ORGANISM_TYPE</th>\n",
       "      <th>ORGANISM_TYPE_DESCRIPTION</th>\n",
       "      <th>IS_COMMON</th>\n",
       "      <th>EXTRACTION_DATE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Nannizzia sp.</td>\n",
       "      <td>f</td>\n",
       "      <td>Fungus</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>WHONET Organisms.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>Escherichia coli O103</td>\n",
       "      <td>-</td>\n",
       "      <td>Gram-negative</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>WHONET Organisms.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>Salmonella Typhimurium DT 104</td>\n",
       "      <td>-</td>\n",
       "      <td>Gram-negative</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>WHONET Organisms.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>Escherichia coli O111</td>\n",
       "      <td>-</td>\n",
       "      <td>Gram-negative</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>WHONET Organisms.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>Neisseria meningitidis, serogroup W135</td>\n",
       "      <td>-</td>\n",
       "      <td>Gram-negative</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>WHONET Organisms.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGANISM_CODE                           ORGANISM_NAME ORGANISM_TYPE  \\\n",
       "0                                         Nannizzia sp.             f   \n",
       "1           103                   Escherichia coli O103             -   \n",
       "2           104           Salmonella Typhimurium DT 104             -   \n",
       "3           111                   Escherichia coli O111             -   \n",
       "4           135  Neisseria meningitidis, serogroup W135             -   \n",
       "\n",
       "  ORGANISM_TYPE_DESCRIPTION IS_COMMON EXTRACTION_DATE           DATA_SOURCE  \n",
       "0                    Fungus        No      2025-05-26  WHONET Organisms.txt  \n",
       "1             Gram-negative        No      2025-05-26  WHONET Organisms.txt  \n",
       "2             Gram-negative        No      2025-05-26  WHONET Organisms.txt  \n",
       "3             Gram-negative        No      2025-05-26  WHONET Organisms.txt  \n",
       "4             Gram-negative        No      2025-05-26  WHONET Organisms.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Also saved 85 common organisms to C:\\Personal_Projects\\Astro\\project_resources\\Common_Organisms.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data directly to CSV and Excel\n",
    "if not df_clean.empty:\n",
    "    print(f\"\\nPreparing to save {len(df_clean)} organisms to files...\")\n",
    "    \n",
    "    # Define output paths\n",
    "    output_csv = r'C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.csv'\n",
    "    output_excel = r'C:\\Personal_Projects\\Astro\\project_resources\\Organisms_Final.xlsx'\n",
    "    \n",
    "    # Add metadata\n",
    "    df_clean['EXTRACTION_DATE'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add data source if not already present\n",
    "    if 'DATA_SOURCE' not in df_clean.columns:\n",
    "        df_clean['DATA_SOURCE'] = 'WHONET Organisms.txt'\n",
    "    \n",
    "    # Ensure proper column order\n",
    "    column_order = [\n",
    "        'ORGANISM_CODE', 'ORGANISM_NAME', 'ORGANISM_TYPE', \n",
    "        'ORGANISM_TYPE_DESCRIPTION', 'IS_COMMON', 'EXTRACTION_DATE', 'DATA_SOURCE'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist in our DataFrame\n",
    "    valid_cols = [col for col in column_order if col in df_clean.columns]\n",
    "    \n",
    "    # Create export DataFrame\n",
    "    df_export = df_clean[valid_cols].copy()\n",
    "    \n",
    "    # Sort by organism code\n",
    "    df_export = df_export.sort_values('ORGANISM_CODE').reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV and Excel\n",
    "    df_export.to_csv(output_csv, index=False)\n",
    "    df_export.to_excel(output_excel, index=False)\n",
    "    \n",
    "    print(f\"Successfully saved {len(df_export)} organisms to:\")\n",
    "    print(f\" - CSV: {output_csv}\")\n",
    "    print(f\" - Excel: {output_excel}\")\n",
    "    \n",
    "    # Display sample of final data\n",
    "    print(\"\\nSample of final dataset:\")\n",
    "    display(df_export.head())\n",
    "    \n",
    "    # Create a special extract for common organisms\n",
    "    common_orgs = df_clean[df_clean['IS_COMMON'] == 'Yes'].copy() if 'IS_COMMON' in df_clean.columns else pd.DataFrame()\n",
    "    \n",
    "    if not common_orgs.empty:\n",
    "        common_path = r'C:\\Personal_Projects\\Astro\\project_resources\\Common_Organisms.xlsx'\n",
    "        common_orgs.sort_values('ORGANISM_CODE').reset_index(drop=True).to_excel(common_path, index=False)\n",
    "        print(f\"\\nAlso saved {len(common_orgs)} common organisms to {common_path}\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a828d",
   "metadata": {},
   "source": [
    "## Analyze Organism Data\n",
    "Generate statistics and analysis of the organism data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0eb36b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Organism Data Analysis ===\n",
      "\n",
      "Organism Type Distribution:\n",
      "  Gram-negative: 1114 organisms (37.8%)\n",
      "  Fungus: 478 organisms (16.2%)\n",
      "  Anaerobe: 432 organisms (14.7%)\n",
      "  Gram-positive: 428 organisms (14.5%)\n",
      "  Other: 213 organisms (7.2%)\n",
      "  Unknown: 104 organisms (3.5%)\n",
      "  Bacteria: 96 organisms (3.3%)\n",
      "  Mycobacteria: 81 organisms (2.7%)\n",
      "\n",
      "Common Organisms: 85 (2.9%)\n",
      "\n",
      "Organism Code Analysis:\n",
      "  Average code length: 3.0 characters\n",
      "  Most common lengths: {3: 2945, 0: 1}\n",
      "\n",
      "Most Common First Letters in Codes:\n",
      "  C: 394 organisms\n",
      "  S: 345 organisms\n",
      "  P: 276 organisms\n",
      "  A: 255 organisms\n",
      "  M: 236 organisms\n",
      "\n",
      "Detailed summary saved to C:\\Personal_Projects\\Astro\\project_resources\\organism_data_summary.txt\n",
      "\n",
      "âœ“ Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Analyze the organism data\n",
    "if not df_clean.empty:\n",
    "    print(\"\\n=== Organism Data Analysis ===\")\n",
    "    \n",
    "    # Organism type analysis\n",
    "    if 'ORGANISM_TYPE_DESCRIPTION' in df_clean.columns:\n",
    "        type_counts = df_clean['ORGANISM_TYPE_DESCRIPTION'].value_counts()\n",
    "        print(\"\\nOrganism Type Distribution:\")\n",
    "        for type_name, count in type_counts.items():\n",
    "            if type_name:  # Skip empty values\n",
    "                percentage = count / len(df_clean) * 100\n",
    "                print(f\"  {type_name}: {count} organisms ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Common organisms\n",
    "    if 'IS_COMMON' in df_clean.columns:\n",
    "        common_count = (df_clean['IS_COMMON'] == 'Yes').sum()\n",
    "        print(f\"\\nCommon Organisms: {common_count} ({common_count/len(df_clean)*100:.1f}%)\")\n",
    "    \n",
    "    # Code pattern analysis\n",
    "    if 'ORGANISM_CODE' in df_clean.columns:\n",
    "        code_lengths = df_clean['ORGANISM_CODE'].str.len()\n",
    "        print(\"\\nOrganism Code Analysis:\")\n",
    "        print(f\"  Average code length: {code_lengths.mean():.1f} characters\")\n",
    "        print(f\"  Most common lengths: {code_lengths.value_counts().head(3).to_dict()}\")\n",
    "        \n",
    "        # First letter frequency\n",
    "        first_letters = df_clean['ORGANISM_CODE'].str[0].value_counts()\n",
    "        print(\"\\nMost Common First Letters in Codes:\")\n",
    "        for letter, count in first_letters.head(5).items():\n",
    "            print(f\"  {letter}: {count} organisms\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    summary_file = r'C:\\Personal_Projects\\Astro\\project_resources\\organism_data_summary.txt'\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"WHONET Organism Data Summary - {datetime.now().strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Total organisms: {len(df_clean)}\\n\")\n",
    "        \n",
    "        if 'ORGANISM_TYPE_DESCRIPTION' in df_clean.columns:\n",
    "            f.write(\"\\nOrganism Type Distribution:\\n\")\n",
    "            for type_name, count in type_counts.items():\n",
    "                if type_name:  # Skip empty values\n",
    "                    percentage = count / len(df_clean) * 100\n",
    "                    f.write(f\"  {type_name}: {count} organisms ({percentage:.1f}%)\\n\")\n",
    "        \n",
    "        if 'IS_COMMON' in df_clean.columns:\n",
    "            common_count = (df_clean['IS_COMMON'] == 'Yes').sum()\n",
    "            f.write(f\"\\nCommon Organisms: {common_count} ({common_count/len(df_clean)*100:.1f}%)\\n\")\n",
    "            \n",
    "            # List of common organisms\n",
    "            f.write(\"\\nList of Common Organisms:\\n\")\n",
    "            common_orgs = df_clean[df_clean['IS_COMMON'] == 'Yes']\n",
    "            for _, row in common_orgs.iterrows():\n",
    "                f.write(f\"  {row['ORGANISM_CODE']}: {row['ORGANISM_NAME']}\\n\")\n",
    "    \n",
    "    print(f\"\\nDetailed summary saved to {summary_file}\")\n",
    "else:\n",
    "    print(\"No data to analyze.\")\n",
    "\n",
    "print(\"\\nâœ“ Process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3b5f3",
   "metadata": {},
   "source": [
    "## Instructions for Running This Notebook\n",
    "\n",
    "To successfully process WHONET organism data:\n",
    "\n",
    "1. Make sure the Organisms.txt file is available in the project_resources directory\n",
    "\n",
    "2. Run each cell in order from top to bottom\n",
    "\n",
    "3. The notebook will:\n",
    "   - Load and parse the Organisms.txt file\n",
    "   - Extract organism codes, names, and types\n",
    "   - Optionally supplement with web scraping (disabled by default)\n",
    "   - Save processed data as CSV and Excel files\n",
    "   - Generate analysis and summary\n",
    "\n",
    "4. The outputs will be saved as:\n",
    "   - `Organisms_Final.csv` - Main CSV file with organism codes, names and types\n",
    "   - `Organisms_Final.xlsx` - Excel version of the data\n",
    "   - `organism_data_summary.txt` - Summary statistics\n",
    "   - `Common_Organisms.xlsx` - List of common organisms\n",
    "\n",
    "## Adjusting the Process\n",
    "\n",
    "To enable web scraping supplementation, set `do_web_scraping = True` in the web scraping cell, but note that:\n",
    "\n",
    "- Web scraping requires a working internet connection\n",
    "- You need Chrome and appropriate ChromeDriver installed\n",
    "- The process may take longer with web scraping enabled\n",
    "- Web data will supplement, not replace, the file data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
